#!/usr/bin/env python3
"""
LLM ç®¡ç†å™¨æ¼”ç¤ºè„šæœ¬ - å±•ç¤ºé€šç”¨ LLM æ¥å£é…ç½®çš„ä½¿ç”¨

æ¼”ç¤ºåŠŸèƒ½ï¼š
1. ä»é…ç½®æ–‡ä»¶åŠ è½½å¤šç§ LLM é…ç½®
2. å±•ç¤ºä¸åŒ LLM æä¾›å•†çš„ç»Ÿä¸€è°ƒç”¨
3. åŠ¨æ€æ·»åŠ å’Œç®¡ç† LLM æ¨¡å‹
4. é…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡çš„ä½¿ç”¨ç¤ºä¾‹

Usage:
    python demo_llm_manager.py
    python demo_llm_manager.py --config llm_config.json
    python demo_llm_manager.py --list-models
    python demo_llm_manager.py --test-model openai
"""

import argparse
import sys
import os
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
current_dir = Path(__file__).parent
src_dir = current_dir / "src"
sys.path.insert(0, str(src_dir))

from src.llm_manager import LLMManager, LLMConfig, LLMProvider


def demo_basic_usage():
    """æ¼”ç¤ºåŸºç¡€ç”¨æ³•"""
    print("ğŸ”§ LLM ç®¡ç†å™¨åŸºç¡€ç”¨æ³•æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»º LLM ç®¡ç†å™¨
    manager = LLMManager()
    
    print(f"âœ“ åˆ›å»º LLM ç®¡ç†å™¨æˆåŠŸ")
    print(f"  - é»˜è®¤æ¨¡å‹: {manager.default_model}")
    
    # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
    all_models = manager.list_models()
    print(f"  - æ‰€æœ‰æ¨¡å‹: {', '.join(all_models)}")
    
    # è·å–å¯ç”¨æ¨¡å‹
    available_models = manager.get_available_models()
    print(f"  - å¯ç”¨æ¨¡å‹: {', '.join(available_models)}")
    
    return manager


def demo_text_generation(manager: LLMManager):
    """æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆ"""
    print("\nğŸ“ æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("=" * 30)
    
    prompt = "è¯·ç”¨ä¸€å¥è¯æè¿°äººå·¥æ™ºèƒ½çš„ä½œç”¨"
    
    # ä½¿ç”¨é»˜è®¤æ¨¡å‹
    print(f"ğŸ“ ä½¿ç”¨é»˜è®¤æ¨¡å‹ç”Ÿæˆæ–‡æœ¬...")
    print(f"   æç¤ºè¯: {prompt}")
    
    try:
        result = manager.generate_text(prompt)
        print(f"   ç”Ÿæˆç»“æœ: {result[:100]}...")
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        info = manager.get_model_info()
        print(f"   æ¨¡å‹ä¿¡æ¯: {info['provider']}-{info['model_name']}")
        
    except Exception as e:
        print(f"   ç”Ÿæˆå¤±è´¥: {e}")


def demo_embedding_generation(manager: LLMManager):
    """æ¼”ç¤ºåµŒå…¥å‘é‡ç”Ÿæˆ"""
    print("\nğŸ”¢ åµŒå…¥å‘é‡æ¼”ç¤º")
    print("=" * 30)
    
    text = "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€å‘å±•çš„é‡è¦æ–¹å‘"
    
    try:
        embedding = manager.get_embedding(text)
        print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: {text}")
        print(f"ğŸ”¢ åµŒå…¥å‘é‡é•¿åº¦: {len(embedding)}")
        print(f"ğŸ”¢ å‘é‡ç¤ºä¾‹: [{embedding[0]:.3f}, {embedding[1]:.3f}, ..., {embedding[-1]:.3f}]")
        
    except Exception as e:
        print(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")


def demo_model_management(manager: LLMManager):
    """æ¼”ç¤ºæ¨¡å‹ç®¡ç†"""
    print("\nâš™ï¸ æ¨¡å‹ç®¡ç†æ¼”ç¤º")
    print("=" * 30)
    
    # åŠ¨æ€æ·»åŠ æ¨¡å‹
    print("â• åŠ¨æ€æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹...")
    custom_config = LLMConfig(
        provider="mock",
        model_name="custom-demo-model",
        max_tokens=500,
        temperature=0.9
    )
    
    manager.add_model('custom_demo', custom_config)
    print(f"   âœ“ æ·»åŠ æ¨¡å‹: custom_demo")
    
    # æµ‹è¯•æ–°æ¨¡å‹
    print("ğŸ§ª æµ‹è¯•æ–°æ·»åŠ çš„æ¨¡å‹...")
    try:
        result = manager.generate_text("æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹", model_name='custom_demo')
        print(f"   âœ“ ç”ŸæˆæˆåŠŸ: {result[:50]}...")
        
        info = manager.get_model_info('custom_demo')
        print(f"   ğŸ“Š æ¨¡å‹ä¿¡æ¯: {info}")
        
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    # ç§»é™¤æ¨¡å‹
    print("â– ç§»é™¤è‡ªå®šä¹‰æ¨¡å‹...")
    manager.remove_model('custom_demo')
    print(f"   âœ“ ç§»é™¤å®Œæˆ")


def demo_config_loading():
    """æ¼”ç¤ºé…ç½®åŠ è½½"""
    print("\nğŸ“‹ é…ç½®åŠ è½½æ¼”ç¤º")
    print("=" * 30)
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½
    config_file = Path("llm_config.json")
    if config_file.exists():
        print(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½: {config_file}")
        manager = LLMManager(config_path=str(config_file))
        
        configs = manager.configs
        print(f"   âœ“ åŠ è½½äº† {len(configs)} ä¸ªé…ç½®:")
        for name, config in configs.items():
            print(f"     - {name}: {config.provider} ({config.model_name})")
    else:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        print("   ä½¿ç”¨é»˜è®¤é…ç½®")
        manager = LLMManager()
    
    return manager


def demo_environment_variables():
    """æ¼”ç¤ºç¯å¢ƒå˜é‡é…ç½®"""
    print("\nğŸŒ ç¯å¢ƒå˜é‡é…ç½®æ¼”ç¤º")
    print("=" * 35)
    
    # æ£€æŸ¥å¸¸è§çš„ç¯å¢ƒå˜é‡
    env_vars = [
        'OPENAI_API_KEY',
        'ANTHROPIC_API_KEY', 
        'QWEN_API_KEY',
        'DEEPSEEK_API_KEY'
    ]
    
    found_keys = []
    for var in env_vars:
        value = os.getenv(var)
        if value:
            found_keys.append(var)
            print(f"   âœ“ {var}: {'*' * 8}{value[-4:] if len(value) > 4 else '****'}")
        else:
            print(f"   âŒ {var}: æœªè®¾ç½®")
    
    if found_keys:
        print(f"\nğŸ‰ æ£€æµ‹åˆ° {len(found_keys)} ä¸ªæœ‰æ•ˆçš„ API key")
        print("   å¯ä»¥å°è¯•ä½¿ç”¨ --use-llm å‚æ•°æµ‹è¯•çœŸå®æ¨¡å‹")
    else:
        print("\nğŸ’¡ æç¤º:")
        print("   - è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨çœŸå®çš„ LLM æ¨¡å‹")
        print("   - ä¾‹å¦‚: export OPENAI_API_KEY='your-api-key'")


def test_specific_model(manager: LLMManager, model_name: str):
    """æµ‹è¯•ç‰¹å®šæ¨¡å‹"""
    print(f"\nğŸ§ª æµ‹è¯•ç‰¹å®šæ¨¡å‹: {model_name}")
    print("=" * 40)
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if model_name not in manager.list_models():
            print(f"âŒ æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨")
            print(f"   å¯ç”¨æ¨¡å‹: {', '.join(manager.list_models())}")
            return
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        info = manager.get_model_info(model_name)
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        for key, value in info.items():
            print(f"   {key}: {value}")
        
        # æ£€æŸ¥å¯ç”¨æ€§
        model = manager.get_model(model_name)
        is_available = model.is_available()
        print(f"ğŸ” å¯ç”¨æ€§æ£€æŸ¥: {'âœ“ å¯ç”¨' if is_available else 'âŒ ä¸å¯ç”¨'}")
        
        if is_available:
            # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
            print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
            result = manager.generate_text("è¯´ä¸€å¥é¼“åŠ±çš„è¯", model_name=model_name)
            print(f"   ç»“æœ: {result[:100]}...")
            
            # æµ‹è¯•åµŒå…¥ï¼ˆå¦‚æœæ”¯æŒï¼‰
            try:
                print(f"ğŸ”¢ æµ‹è¯•åµŒå…¥å‘é‡...")
                embedding = manager.get_embedding("æµ‹è¯•æ–‡æœ¬", model_name=model_name)
                print(f"   å‘é‡é•¿åº¦: {len(embedding)}")
            except NotImplementedError:
                print(f"   âš ï¸ è¯¥æ¨¡å‹ä¸æ”¯æŒåµŒå…¥å‘é‡")
            except Exception as e:
                print(f"   âŒ åµŒå…¥æµ‹è¯•å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


def list_all_models(manager: LLMManager):
    """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯"""
    print("\nğŸ“‹ æ‰€æœ‰æ¨¡å‹è¯¦ç»†ä¿¡æ¯")
    print("=" * 40)
    
    models = manager.list_models()
    
    for i, model_name in enumerate(models, 1):
        print(f"\n[{i}] {model_name}")
        print("-" * 20)
        
        try:
            info = manager.get_model_info(model_name)
            model = manager.get_model(model_name)
            is_available = model.is_available()
            
            print(f"   æä¾›å•†: {info.get('provider', 'unknown')}")
            print(f"   æ¨¡å‹å: {info.get('model_name', 'unknown')}")
            print(f"   çŠ¶æ€: {'âœ“ å¯ç”¨' if is_available else 'âŒ ä¸å¯ç”¨'}")
            
            # æ˜¾ç¤ºå…¶ä»–é…ç½®ä¿¡æ¯
            for key, value in info.items():
                if key not in ['provider', 'model_name']:
                    print(f"   {key}: {value}")
                    
        except Exception as e:
            print(f"   âŒ è·å–ä¿¡æ¯å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="LLM ç®¡ç†å™¨æ¼”ç¤ºè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºç¡€æ¼”ç¤º
  python demo_llm_manager.py
  
  # æŒ‡å®šé…ç½®æ–‡ä»¶
  python demo_llm_manager.py --config llm_config.json
  
  # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
  python demo_llm_manager.py --list-models
  
  # æµ‹è¯•ç‰¹å®šæ¨¡å‹
  python demo_llm_manager.py --test-model mock
  
  # æ£€æŸ¥ç¯å¢ƒå˜é‡
  python demo_llm_manager.py --check-env
        """
    )
    
    parser.add_argument(
        '--config',
        help='æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--list-models',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯'
    )
    
    parser.add_argument(
        '--test-model',
        help='æµ‹è¯•ç‰¹å®šæ¨¡å‹'
    )
    
    parser.add_argument(
        '--check-env',
        action='store_true',
        help='æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®'
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ LLM ç®¡ç†å™¨æ¼”ç¤º")
    print("=" * 50)
    
    try:
        if args.check_env:
            demo_environment_variables()
            return
        
        # åˆ›å»ºç®¡ç†å™¨
        if args.config:
            print(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
            manager = LLMManager(config_path=args.config)
        else:
            manager = demo_config_loading()
        
        if args.list_models:
            list_all_models(manager)
            return
        
        if args.test_model:
            test_specific_model(manager, args.test_model)
            return
        
        # å®Œæ•´æ¼”ç¤º
        demo_basic_usage()
        demo_text_generation(manager)
        demo_embedding_generation(manager)
        demo_model_management(manager)
        demo_environment_variables()
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ æç¤º:")
        print("  - ä½¿ç”¨ --list-models æŸ¥çœ‹æ‰€æœ‰æ¨¡å‹")
        print("  - ä½¿ç”¨ --test-model <name> æµ‹è¯•ç‰¹å®šæ¨¡å‹")
        print("  - è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨çœŸå®çš„ LLM æ¨¡å‹")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ æ¼”ç¤ºå‡ºé”™: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()