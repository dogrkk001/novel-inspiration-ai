#!/usr/bin/env python3
"""
æ¼”ç¤ºå®Œæ•´æµç¨‹è„šæœ¬ - ä»è¾“å…¥åˆ°æ£€ç´¢çš„ç«¯åˆ°ç«¯æ¼”ç¤º

æ¼”ç¤ºæµç¨‹ï¼š
1. ä»æ–‡ä»¶è¯»å–æ ·ä¾‹æ–‡æœ¬
2. ä½¿ç”¨è¾“å…¥æ¨¡å—è¿›è¡Œæ–‡æœ¬åˆ‡åˆ†
3. ä½¿ç”¨çµæ„Ÿæå–æ¨¡å—æå–åˆ›ä½œçµæ„Ÿï¼ˆæ”¯æŒ MockLLM å’ŒçœŸå® LLMï¼‰
4. å°†æå–ç»“æœå­˜å‚¨åˆ° SQLite æ•°æ®åº“
5. ä½¿ç”¨æ£€ç´¢æ¨¡å—è¿›è¡Œå…³é”®è¯æœç´¢å¹¶å±•ç¤ºç»“æœ

Usage:
    python demo_pipeline.py --input ../data/sample_novel.txt --db test_pipeline.db --keyword "æ­¦åŠŸ"
    python demo_pipeline.py --input ../data/sample_novel.txt --db test_pipeline.db --keyword "å‹‡æ°”" --use-llm
"""

import argparse
import sys
import os
from pathlib import Path
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from src.input_module import InputModule
from src.extractor import InspirationExtractor
from src.database import save_batch, DatabaseError
from src.search import search_inspirations, SearchError
from src.search_enhancement import SearchEnhancement, SearchEnhancementError
from src.llm_manager import LLMManager, LLMConfig


class PipelineError(Exception):
    """ç®¡é“æ‰§è¡Œé”™è¯¯"""
    pass


class DemoPipeline:
    """æ¼”ç¤ºç®¡é“ä¸»ç±»"""
    
    def __init__(self, db_path: str, use_llm: bool = False, model_name: Optional[str] = None,
                 use_semantic_search: bool = False):
        """
        åˆå§‹åŒ–æ¼”ç¤ºç®¡é“
        
        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            use_llm: æ˜¯å¦ä½¿ç”¨çœŸå®çš„ LLMï¼ˆéœ€è¦ API keyï¼‰
            model_name: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç§°
            use_semantic_search: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰æ£€ç´¢å¢å¼º
        """
        self.db_path = db_path
        self.use_llm = use_llm
        self.model_name = model_name
        self.use_semantic_search = use_semantic_search
        self.input_module = InputModule()
        
        # åˆå§‹åŒ– LLM ç®¡ç†å™¨
        self.llm_manager = LLMManager()
        
        # åˆå§‹åŒ–æ£€ç´¢å¢å¼ºæ¨¡å—
        if use_semantic_search:
            self.search_enhancement = SearchEnhancement(db_path, self.llm_manager)
        else:
            self.search_enhancement = None
        
        # åˆå§‹åŒ–æå–å™¨
        if use_llm and model_name:
            # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
            try:
                llm = self.llm_manager.get_model(model_name)
                print(f"âœ“ ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {model_name}")
            except ValueError:
                print(f"âš  æŒ‡å®šæ¨¡å‹ '{model_name}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
                llm = self._get_best_available_llm()
        elif use_llm:
            # ä½¿ç”¨æœ€ä½³å¯ç”¨çš„ LLM
            llm = self._get_best_available_llm()
        else:
            # ä½¿ç”¨ MockLLM
            llm = self.llm_manager.get_model('mock')
        
        # åˆ›å»ºå…¼å®¹çš„ LLM å®ä¾‹ï¼ˆä½¿ç”¨è€çš„æ¥å£ï¼‰
        if hasattr(llm, 'generate_text'):
            # æ–°çš„ LLM ç®¡ç†å™¨æ¥å£ï¼Œéœ€è¦é€‚é…å™¨
            from src.extractor import LLMInterface as ExtractorLLMInterface
            
            class LLMAdapter(ExtractorLLMInterface):
                def __init__(self, new_llm):
                    self.new_llm = new_llm
                
                def generate(self, prompt: str) -> str:
                    return self.new_llm.generate_text(prompt)
                
                def get_model_name(self) -> str:
                    info = self.new_llm.get_model_info()
                    return f"{info.get('provider', 'unknown')}-{info.get('model_name', 'unknown')}"
            
            adapted_llm = LLMAdapter(llm)
        else:
            # æ—§çš„æ¥å£ï¼Œç›´æ¥ä½¿ç”¨
            adapted_llm = llm
        
        self.extractor = InspirationExtractor(llm=adapted_llm)  # type: ignore
        
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ•°æ®åº“è·¯å¾„: {db_path}")
        print(f"  - LLMæ¨¡å¼: {'çœŸå®LLM' if use_llm else 'MockLLM'}")
        print(f"  - æ¨¡å‹: {llm.get_model_info()['model_name']}")
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
        available_models = self.llm_manager.get_available_models()
        print(f"  - å¯ç”¨æ¨¡å‹: {', '.join(available_models)}")
    
    def _get_best_available_llm(self):
        """è·å–æœ€ä½³å¯ç”¨çš„ LLM"""
        available = self.llm_manager.get_available_models()
        
        # ä¼˜å…ˆçº§é¡ºåºï¼šOpenAI > Claude > Qwen > DeepSeek > Mock
        preference_order = ['openai', 'claude', 'qwen', 'deepseek', 'mock']
        
        for preferred in preference_order:
            if preferred in available:
                print(f"âœ“ ä½¿ç”¨å¯ç”¨æ¨¡å‹: {preferred}")
                return self.llm_manager.get_model(preferred)
        
        # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨ mock
        print("âš  æ²¡æœ‰å¯ç”¨çš„çœŸå® LLMï¼Œä½¿ç”¨ MockLLM")
        return self.llm_manager.get_model('mock')
    
    def run(self, input_file: str, keyword: str) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ¼”ç¤ºæµç¨‹
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            keyword: æ£€ç´¢å…³é”®è¯
            
        Returns:
            åŒ…å«å„æ­¥éª¤ç»“æœçš„å­—å…¸
        """
        results = {
            'input_file': input_file,
            'keyword': keyword,
            'text_chunks': [],
            'inspirations': [],
            'saved_count': 0,
            'search_results': []
        }
        
        try:
            # æ­¥éª¤1: è¯»å–å’Œåˆ‡åˆ†æ–‡æœ¬
            print(f"\nğŸ“– æ­¥éª¤1: è¯»å–æ–‡ä»¶ {input_file}")
            if not Path(input_file).exists():
                raise PipelineError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            
            process_result = self.input_module.process_file(input_file)
            text_chunks = process_result['chunks']
            results['text_chunks'] = text_chunks
            
            print(f"âœ“ æ–‡ä»¶è¯»å–å®Œæˆ")
            print(f"  - åˆ‡åˆ†å—æ•°: {len(text_chunks)}")
            if text_chunks:
                avg_length = sum(len(chunk.get('content', '')) for chunk in text_chunks) / len(text_chunks)
                print(f"  - å¹³å‡å—é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
            
            # æ­¥éª¤2: æå–çµæ„Ÿ
            print(f"\nğŸ¯ æ­¥éª¤2: æå–åˆ›ä½œçµæ„Ÿ")
            inspirations = []
            
            for i, chunk in enumerate(text_chunks, 1):
                print(f"  å¤„ç†ç¬¬ {i}/{len(text_chunks)} å—...")
                
                try:
                    # æå–çµæ„Ÿ
                    inspiration_data = self.extractor.extract_inspiration(chunk['content'])
                    
                    # å‡†å¤‡ä¿å­˜åˆ°æ•°æ®åº“çš„æ•°æ®æ ¼å¼
                    db_data = {
                        'source_file': input_file,
                        'chapter': chunk.get('title', f'ç¬¬{i}å—'),
                        'raw_text': chunk['content'][:500],  # é™åˆ¶åŸæ–‡é•¿åº¦
                        'idea': inspiration_data['theme'],
                        'tags': f"{inspiration_data['world_elements']}, {', '.join(inspiration_data['characters'])}"[:200]  # é™åˆ¶æ ‡ç­¾é•¿åº¦
                    }
                    
                    inspirations.append(db_data)
                    
                except Exception as e:
                    print(f"    è­¦å‘Š: æå–ç¬¬ {i} å—æ—¶å‡ºé”™: {e}")
                    continue
            
            results['inspirations'] = inspirations
            print(f"âœ“ çµæ„Ÿæå–å®Œæˆ")
            print(f"  - æˆåŠŸæå–: {len(inspirations)} æ¡çµæ„Ÿ")
            
            # æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®åº“
            print(f"\nğŸ’¾ æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®åº“")
            if inspirations:
                try:
                    saved_ids = save_batch(inspirations, self.db_path)
                    results['saved_count'] = len(saved_ids)
                    
                    print(f"âœ“ æ•°æ®ä¿å­˜å®Œæˆ")
                    print(f"  - ä¿å­˜è®°å½•æ•°: {len(saved_ids)}")
                    print(f"  - è®°å½•IDèŒƒå›´: {min(saved_ids)} - {max(saved_ids)}")
                
                except DatabaseError as e:
                    raise PipelineError(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            else:
                print("âš  æ²¡æœ‰æœ‰æ•ˆçš„çµæ„Ÿæ•°æ®éœ€è¦ä¿å­˜")
            
            # æ­¥éª¤4: æ£€ç´¢æ¼”ç¤º
            print(f"\nğŸ” æ­¥éª¤4: æ£€ç´¢æ¼”ç¤º")
            try:
                if self.use_semantic_search and self.search_enhancement:
                    # ä½¿ç”¨è¯­ä¹‰æ£€ç´¢å¢å¼º
                    print(f"  ä½¿ç”¨è¯­ä¹‰æ£€ç´¢å¢å¼ºæ¨¡å—")
                    
                    # æ„å»ºå‘é‡ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    stats = self.search_enhancement.get_index_stats()
                    if not stats['index_complete']:
                        print(f"  æ„å»ºå‘é‡ç´¢å¼•...")
                        processed = self.search_enhancement.build_index()
                        print(f"  âœ“ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼šå¤„ç† {processed} æ¡è®°å½•")
                    
                    # æ‰§è¡Œä¸åŒç±»å‹çš„æ£€ç´¢
                    print(f"\n  ğŸ” å…³é”®è¯æ£€ç´¢ï¼š")
                    keyword_results = self.search_enhancement.search(keyword, mode='keyword', limit=5)
                    print(f"    åŒ¹é…ç»“æœ: {len(keyword_results)} æ¡")
                    
                    print(f"  ğŸ§  è¯­ä¹‰æ£€ç´¢ï¼š")
                    semantic_results = self.search_enhancement.search(keyword, mode='semantic', limit=5)
                    print(f"    åŒ¹é…ç»“æœ: {len(semantic_results)} æ¡")
                    
                    print(f"  ğŸŒ æ··åˆæ£€ç´¢ï¼š")
                    hybrid_results = self.search_enhancement.search(keyword, mode='hybrid', limit=10)
                    search_results = hybrid_results
                    
                    print(f"\nâœ“ è¯­ä¹‰æ£€ç´¢å®Œæˆ")
                    print(f"  - å…³é”®è¯åŒ¹é…: {len(keyword_results)} æ¡")
                    print(f"  - è¯­ä¹‰åŒ¹é…: {len(semantic_results)} æ¡")
                    print(f"  - æ··åˆæ£€ç´¢: {len(hybrid_results)} æ¡")
                    
                    # å±•ç¤ºæ··åˆæ£€ç´¢ç»“æœ
                    if hybrid_results:
                        print(f"\nğŸ† æ··åˆæ£€ç´¢ç»“æœé¢„è§ˆï¼š")
                        for i, result in enumerate(hybrid_results[:5], 1):
                            print(f"  [{i}] ID: {result['id']} | ç»¼åˆè¯„åˆ†: {result['composite_score']:.3f}")
                            print(f"      æ¥æº: {result['source_file']}")
                            print(f"      åˆ›æ„: {result['idea'][:60]}...")
                            print(f"      è¯„åˆ†æ˜ç»†: å…³é”®è¯={result['keyword_score']:.3f}, æ¨¡ç³Š={result['fuzzy_score']:.3f}, è¯­ä¹‰={result['semantic_score']:.3f}")
                            print()
                    
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢
                    print(f"  ä½¿ç”¨ä¼ ç»Ÿå…³é”®è¯æ£€ç´¢")
                    search_results = search_inspirations(self.db_path, keyword, limit=10)
                    
                    print(f"âœ“ æ£€ç´¢å®Œæˆ")
                    print(f"  - å…³é”®è¯: '{keyword}'")
                    print(f"  - åŒ¹é…ç»“æœ: {len(search_results)} æ¡")
                    
                    # å±•ç¤ºå‰å‡ æ¡ç»“æœ
                    if search_results:
                        print(f"\nğŸ¯ æ£€ç´¢ç»“æœé¢„è§ˆï¼š")
                        for i, result in enumerate(search_results[:5], 1):
                            print(f"  [{i}] ID: {result['id']}")
                            print(f"      æ¥æº: {result['source_file']}")
                            print(f"      ç« èŠ‚: {result['chapter']}")
                            print(f"      åˆ›æ„: {result['idea'][:80]}...")
                            print(f"      æ ‡ç­¾: {result['tags'][:50]}..." if result['tags'] else "      æ ‡ç­¾: æ— ")
                            print()
                    else:
                        print(f"  âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '{keyword}' çš„è®°å½•")
                        
                results['search_results'] = search_results
            
            except SearchError as e:
                print(f"  âŒ æ£€ç´¢å¤±è´¥: {e}")
                results['search_results'] = []
            
            return results
            
        except Exception as e:
            raise PipelineError(f"æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å°è¯´çµæ„Ÿæå–ç³»ç»Ÿ - å®Œæ•´æµç¨‹æ¼”ç¤º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨ MockLLM è¿›è¡Œæ¼”ç¤º
  python demo_pipeline.py --input ../data/sample_novel.txt --db demo.db --keyword "æ­¦åŠŸ"
  
  # ä½¿ç”¨çœŸå® LLMï¼ˆéœ€è¦è®¾ç½® API keyï¼‰
  python demo_pipeline.py --input ../data/sample_novel.txt --db demo.db --keyword "å‹‡æ°”" --use-llm
  
  # ä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶
  python demo_pipeline.py --input /path/to/novel.txt --db custom.db --keyword "å‹æƒ…"
        """
    )
    
    parser.add_argument(
        '--input', 
        required=True,
        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (æ”¯æŒ TXT æ ¼å¼)'
    )
    
    parser.add_argument(
        '--db', 
        required=True,
        help='SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--keyword', 
        required=True,
        help='æ£€ç´¢å…³é”®è¯'
    )
    
    parser.add_argument(
        '--use-llm', 
        action='store_true',
        help='ä½¿ç”¨çœŸå®çš„ LLMï¼ˆéœ€è¦ API keyï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ MockLLM'
    )
    
    parser.add_argument(
        '--model', 
        help='æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç§° (openai, claude, qwen, deepseek, mock)'
    )
    
    parser.add_argument(
        '--semantic', 
        action='store_true',
        help='ä½¿ç”¨è¯­ä¹‰æ£€ç´¢å¢å¼ºåŠŸèƒ½ï¼ˆåŸºäºå‘é‡ç›¸ä¼¼åº¦ï¼‰'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not Path(args.input).exists():
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)
    
    print("ğŸš€ å°è¯´çµæ„Ÿæå–ç³»ç»Ÿ - å®Œæ•´æµç¨‹æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¹¶è¿è¡Œç®¡é“
        pipeline = DemoPipeline(
            db_path=args.db, 
            use_llm=args.use_llm, 
            model_name=args.model,
            use_semantic_search=args.semantic
        )
        results = pipeline.run(input_file=args.input, keyword=args.keyword)
        
        # æ€»ç»“æŠ¥å‘Š
        print("\nğŸ“Š æ‰§è¡Œæ€»ç»“")
        print("=" * 30)
        print(f"âœ“ è¾“å…¥æ–‡ä»¶: {results['input_file']}")
        print(f"âœ“ æ–‡æœ¬å—æ•°: {len(results['text_chunks'])}")
        print(f"âœ“ æå–çµæ„Ÿ: {len(results['inspirations'])} æ¡")
        print(f"âœ“ ä¿å­˜è®°å½•: {results['saved_count']} æ¡")
        print(f"âœ“ æ£€ç´¢ç»“æœ: {len(results['search_results'])} æ¡")
        print(f"âœ“ æ•°æ®åº“æ–‡ä»¶: {args.db}")
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        
    except PipelineError as e:
        print(f"\nâŒ ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print(f"\nâ¹ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ æœªé¢„æœŸé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()